# Word Representation for NLP [Word Embedding]

- Machines cannot understand words. Therefore, when we want to train a machine learning model on text, we must first represent this text as vectors.
- Vectors are derived from textual data, in order to reflect various linguistic properties of the 
text.

 This process of representing text as a vector is called **word embedding**.

### **The 2 main approaches for word embedding are:**
   - **Frequency Based Embedding**
     - One-Hot Encoded vector
     - Bag of Word (BOW) Count Vector
     -  Term Frequency- Inverse Document frequency (TF-IDF) Vector
     -  Co-Occurrence Vector
   - **Prediction Based Embedding**
     -  Word2Vec
        - CBOW
        - Skipgram
     - Glove
